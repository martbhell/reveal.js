<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Is Placement a Replacement?</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
					<section>
						This slideset is made using <a href="https://github.com/hakimel/reveal.js/" target="_blank">reveal.js</a>.
<br />

You can change slides with the *arrow keys*.
<br />

You can get an overview of all the slides by pressing *esc*.
<br />

Press **↓** to see an index of topics.
<br />
Press **→** one or more times to select topic.

					</section>
					<section data-background="topic_background.png" data-background-size="contain">
						<br />Placement <br /> <font size="+2">2019 <br /> Johan Guldmyr & CSC Pouta Cloud Team</font>
					</section>
					<section>
						Credits to CSC Pouta Team & Boris in openstack-operator channel<br />
						and various placement videos :)
					</section>
					<section>
						<ul>
							<li>JIRA task about adding it: <a href="https://jira.csc.fi/browse/CCCP-2598">CCCP-2598</a></li>
							<li>JIRA task about making some changes afterwards: <a href="https://jira.csc.fi/browse/CCCP-2605">CCCP-2605</a></li>
							<li>JIRA task about fixing some errors: <a href="https://jira.csc.fi/browse/CCCP-2611">CCCP-2611</a></li>
						</ul>
						<br><br>
						These slides: <h4><a href="https://cakeplacement.object.pouta.csc.fi/index.html">https://cakeplacement.object.pouta.csc.fi/index.html</a></h4>
					</section>
					<section>
						<ul>
							<li>Documentation: <a href="https://docs.openstack.org/placement/latest/">Latest docs</a> </li>
							<li>List of changes: <a href="https://docs.openstack.org/nova/rocky/user/placement.html">Rocky Nova User Placement</a> </li>
							<li>For newton: ??? </li>
							<li>For ocata:
							<a href="https://docs.openstack.org/ocata/install-guide-ubuntu/nova-controller-install.html">Deployment Guide</a> for Nova Ubuntu was useful (had example of endpoint) </li>

							<li>	<a href="http://github.com/openstack/nova">github for openstack/nova</a></li>
							<li><a href="https://github.com/openstack/placement">openstack/placement</a> after extraction</li>

					</section>
					<section>
						<h2> Vid(eoand)yous:<h2/> <br />
						1: <a href="https://www.openstack.org/videos/summits/vancouver-2018/placement-present-and-future-in-nova-and-beyond">placement present future in nova and beyond </a><br />
						2: <a href="https://www.youtube.com/watch?v=LVkknWuGq_I&feature=youtu.be&list=PL0vkr2Qkh9oM6-avm8kqlk0eEhk2x7zhI&t=1595">Video about what is happening in Newton and Ocata</a><br />

					</section>
					<section>
						Two things:<br/>
						<h4>Placement and its API</h4>
						<h4>Nova's Resource Tracker</h4>
					</section>
					<section>
						In <b style=color:green>Newton: nova-scheduler </b></font> nova/scheduler/filter_scheduler.py does not contain 'placement'<br />
						In <b style=color:green>Newton: nova-compute </b></font> talk to placement and update their resource providers there. <br />
						In <b style=color:green>Ocata: nova-scheduler </b> will use old method until all nova-computes are ocata. <i>(Not anymore in Pike)</i><br/>
						In <b style=color:green>Ocata: nova-compute </b> will not start without [placement]<br />
					</section>
					<section>
						ROCKY <br>
						<a href="https://docs.openstack.org/nova/rocky/user/architecture.html"> rocky nova architecture has a diagram</a>
						<img height=auto src="https://docs.openstack.org/nova/rocky/_images/architecture.svg">
					</section>
					<section>
						<h4>Things puppet does now</h4>
						openstack-nova-placement-api.rpm </br>
						WSGI setup<br />
						openstack admin user <br/>
						service catalog and endpoints <br />
						nova.conf [placement] (both API & compute)<br />
						<i>(cpu,ram_disk)_allocation_ratios in nova.conf</i><br />
					</section>
					<section>
						<section>
							<h4> openstack resource provider list</h4>
								placement requires admin credentials<br>
								<code>
						$ pip install osc-placement 
					</code>
						<br />
						|<br>
						|<br>
						V<br>
						</section>
						<section>
							<pre>
| uuid (same as in compute_nodes table in nova)| name           | generation |
+--------------------------------------+------------------------+------------+
| bb30e381-UUID-4749-b439-Q | n2.cloud.example.org        |          1 |
| 8b225ecb-UUID-471c-b099-Q | n3.cloud.example.org        |          1 |
| e5a3d46d-UUID-43ce-9ceb-Q | foo.cloud.example.org 	  |          1 |
| a996fb27-UUID-45ee-b98c-Q | foo3.cloud.example.org      |         17 |
| 0003354d-UUID-4604-ba36-Q | foo5.cloud.example.org      |         11 |
| 4911d9f6-UUID-471e-b779-Q | q.cloud.example.org         |          1 |
+--------------------------------------+------------------------+------------+
							</pre>
						</section>
						<section>
							Can you see something interesting with the numbers in the following tables where we list inventory of two resource providers?
						</section>
						<section>
<pre>
openstack resource provider inventory list 0003354d-UUID-4604-ba36-TAITOP2 -f table -c resource_class -c allocation_ratio -c max_unit -c reserved -c step_size -c total
+----------------+------------------+----------+----------+-----------+--------+
| resource_class | allocation_ratio | max_unit | reserved | step_size |  total |
+----------------+------------------+----------+----------+-----------+--------+
| VCPU           |             16.0 |       48 |        0 |         1 |     48 |
| MEMORY_MB      |              1.0 |   130946 |      512 |         1 | 130946 |
| DISK_GB        |              1.0 |      855 |        0 |         1 |    855 |
+----------------+------------------+----------+----------+-----------+--------+
</pre>
						</section>
						<section>
							<pre>
$ openstack resource provider inventory list a996fb27-UUID-45ee-b98c-oversubc64C  -f table -c resource_class -c allocation_ratio -c max_unit -c reserved -c step_size -c total 
+----------------+------------------+----------+----------+-----------+---------+
| resource_class | allocation_ratio | max_unit | reserved | step_size |   total |
+----------------+------------------+----------+----------+-----------+---------+
| VCPU           |             16.0 |       40 |        0 |         1 |      40 |
| MEMORY_MB      |              1.0 |   522949 |    44000 |         1 |  522949 |
| DISK_GB        |              1.0 |  2656144 |        0 |         1 | 2656144 |
+----------------+------------------+----------+----------+-----------+---------+
</pre>
</section>
					</section>
					<section>
						<a href="https://docs.openstack.org/releasenotes/nova/ocata.html">Ocata and Nova</a> <br />
						You can safely remove the AggregateCoreFilter, AggregateRamFilter, and AggregateDiskFilter from your [filter_scheduler]enabled_filters and you do not need to replace them with any other core/ram/disk filters. The placement query in the FilterScheduler takes care of the core/ram/disk filtering, so CoreFilter, RamFilter, and DiskFilter are redundant. <br />	
						<br />
						We don't use DiskFilter but we have RamFilter and AggregateCoreFilter
						<br />
						Nova aggregate metadata does not propagate into placement resource providers in Ocata.
					</section>

					<section>
						Some PCI data also shows up on GPU nodes
						<code>
					pci_stats=[PciDevicePool(count=4,numa_node=0,product_id='13bd',tags={dev_type='type-PCI'},vendor_id='10de'), PciDevicePool(count=4,numa_node=1,product_id='13bd',tags={dev_type='type-PCI'},vendor_id='10de')]
					</code>
					</section>
					<section>
						Post Ocata Thoughts 
						<section>
							<br>
							<h3>Pike 1/2</h3>
							- Nova FilterScheduler now requests allocation candidates during scheduling.<br />
							- <a href="https://docs.openstack.org/releasenotes/nova/pike.html">Some bugs fixed around live migration and evacuate to specific hosts</a><br />
							- an allocation request is still made and if it fails migration/evacuate fails. <br />
						|<br>
						|<br>
						V<br>
						</section>
						<section>
							<br>
							<h3>pike 2/2</h3>
							- benefits when using multiple schedulers <br />
							- traits are added<br />
						</section>
						<section>
							<br>
							<h4>Queens 1/2</h4>
							- placement must be upgraded before nova-scheduler<br/><br>
							  - <a title="found that it came from queens by grepping openstack/nova git repo" href="https://docs.openstack.org/placement/latest/configuration/config.html">placement.conf randomize_allocation_candidates introduced in queens</a><br />
							 If False, allocation candidates are returned in a deterministic but undefined order. That is, all things being equal, two requests for allocation candidates will return the same results in the same order; but no guarantees are made as to how that order is determined.<br />
						 </section>
						 <section>
							 <br>
							 <h4>Queens 2/2</h4>
							 <ul>
								 <li> Added a required (traits) to the GET allocation_candidates </li>
								 <li> Traits extra specs can be added to flavours like: </li>
								 <code>
							 trait:HW_CPU_X86_AVX2=required
							 trait:STORAGE_DISK_SSD=required
						 </code>
						 <li> [schedulers]/max_placement_results defaults to 1000 </li> 
						 </ul>
						 </section>
						 <section>
							 <br>
							 <h4>rocky 1/2</h4>
							 <ul>
								 <li> placement support for rbac</li>
								 <li> has "member_of" - only return resource providers that are part of an aggregate</li>
								 <li> nova-api now deletes resource providers when updating instances, nova-computes and mirror nova aggregates </li>
								 <li> can now configure a [placement_database] </li>
								 <li> changing this and running "nova-manage api_db sync" will create tables but won't sync any data </li>
								 <li> up to each deployment to do this? </li>
						 </ul>
						 </section>
						 <section>
							 <h4>rocky 2/2 </h4>
							 <ul>
							 <li>nova now supports reporting cpu traits of libvirt guests to placement </li>
							 <li>nova scheduler can now use placement for better query with tenant-restricted aggregatse. Requires a new setting </li>
						 </ul>
						 </section>
						 <section>
							 <h4>Stein/Train </h4>
							 <ul>
							 <li>- extracted:</li>
							 <li>so own RPM, own database, openstack/placement and own openstack/puppet-placement</li>
						 </ul>
						</section>
					</section>
					<section>
						<h4>Placement Things: </h4>
						<ul>
							<li>resource providers </li>
							<li>inventory </li>
							<li>allocations </li>
							<li><b style=color:green>"Custom resource classes</b> are specific to a deployment and represent types of quantitative resources that are not interoperable between OpenStack clouds.</li>
							<li>aggregates </li> 
						</ul>
					</section>
					<section>
						If a compute gets a new UUID == admin work to remove resource provider. <a href="https://developer.openstack.org/api-ref/placement/" title="api-ref/placement">Can't have duplicate names </a><br />
						If a compute gets reinstalled - no extra work needed, will keep using compute node's UUID from compute_nodes nova table in placement<br />
						<h4> logs </h4>
						<ul>
							<li> /var/log/httpd/placement* </li>
							<li> /var/log/nova/nova-placement* </li>
							<li> /var/log/nova/nova-compute* </li>
						</ul> <br />
						 logs aren't populated until there's been a request to the API (== logs are on node withe VIP)
					</section>
					<section>
						<section>
							<h2>errors seen</h2>
							|<br />
							|<br />
							|<br />
							V<br />
						</section>
						<section>
							<h4> Over Capacity </h4>
Over capacity for DISK_GB on resource provider UUIDHEREPLZTAITOP2NODE. Needed: 80, Used: 800, Capacity: 855.0
						<br><br>
						Over capacity for DISK_GB on resource provider UUIDHEREPLZCPOUTAIO. Needed: <b style=color:green>720</b>, Used: <b style=color:green>1440</b>, Capacity: <b style=color:red>2133.0</b>
						<br />
						<br />
						Set disk_allocation_ratio in nova.
						</section>
						<section>
							<h4> Violate Inventory Constraint </h4>
							Allocation for DISK_GB on resource provider fa6a0aa3-UUID-4da0-a895-IO/TB/broadwell violates min_unit, max_unit, or step_size. Requested: <b style=color:red>1730</b>, min_unit: 1, max_unit: <b style=color:green>1687</b>, step_size: 1
						<br><br>
						Fix flavour?<br />

						</section>
						<section>
							<a href="https://bugs.launchpad.net/nova/+bug/1681658">violate inventory constraints</a> openstack bugfix won'tfix<br><br>
							InvalidAllocationConstraintsViolated: Unable to create allocation for 'DISK_GB' on resource provider '0bb7777c-UUID-4616-8890-IO/TB/broadwell'. <b style=color:red>The requested amount would violate inventory constraints.</b>
						</section>
						<section>
							Thin Provisioning is done with LVM. No need to zero out disks (source Kalle :)
						</section>
						<section>
							What should we do about these issues with the IO/TB/broadwell TB.3.480RAM?
						</section>
					</section>
					<section>
						FIN
					</section>
				
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
